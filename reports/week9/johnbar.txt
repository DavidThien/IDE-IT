-----------------------
GOALS FROM PREVIOUS WEEK
-----------------------
	- Finish converting false negative list to runnable tests 
		- This will likely require reworking our mock testing framework
		- getter/setters
		- correct indentation
	- Design and implement clean way to store one list of featureID strings throughout our plugin
		- easy to reference when needed with evaluators
		- one location so if we add or change the strings, there is only one location for all of them
	- Update report to include analysis information regarding our success metrics
	- Clean up and finalize the getter/setter evaluation function
	- Find another Eclipse feature we could support and design a working evaluation function for it
		- Ideally something that uses a config instead of a hotkey
	- Determine what we're doing with the remove import suggestion 
		- When exactly should it trigger and what triggers it
		- Trigger upon manually deleting an unused import will not work 
			- Doc change to manually delete an entire line and to use the hotkey to remove an import are identical internally

-----------------------
PROGRESS MADE THIS WEEK
-----------------------
	- Finalized getter/setter evaluation function
		- Includes parsing AST from document and using ASTVisitor interface to find specific information
	- Convert list of user actions to runnable tests
		- Correct indentation
			- unit tests, positives, and negatives
		- getter/setter
			- unit tests (so far)
	- Designed and implemented clean way to store a single list of all featureID strings
	- Updated report to clarify evaluation methodology
	- Updated user manual to reflect featureID string usage
	
----------------------------------
PLANS AND GOALS FOR FOLLOWING WEEK
----------------------------------
	- Update report to analyze evaluation results 
		- comparing how many false negatives we pass / fail
		- create graphs for all current evaluation functions
		- what IDE-IT is good at and what it's bad at
	- Meet with FE to finalize project as a whole
	- Update report and user manual with feedback
	- Create script that runs tests we should fail
		- User actions that could be caught by our evaluation functions when we don't want them to be
		- aka false positives
	- Clean up repo code
		- remove all debug println statements
	- Fix getter/setter bug
		- easy to reproduce evaluation trigger when it shouldn't
